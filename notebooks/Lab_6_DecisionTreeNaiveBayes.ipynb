{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nickplas/Intro_to_ML_24-25/blob/main/notebooks/Lab_6_DecisionTreeNaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lwvjp5PUb6o"
   },
   "source": [
    "# Classification with Decision Trees and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tpkP4x7pUkdX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCghdR1LUqSZ"
   },
   "source": [
    "Import and pre-process the data using `pandas`.\n",
    "Data source: https://archive-beta.ics.uci.edu/dataset/73/mushroom\n",
    "\n",
    "For practicality, it also on github (csv format, with features names): https://github.com/LucaPennella/Intro_to_ML_23-24/blob/main/data/Mushroom.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Px4uD726Teeg",
    "outputId": "842a8b01-0de1-4869-d28e-390deaca6d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Either the file is missing or not readable, download it\n",
      "--2024-11-12 15:52:31--  https://raw.githubusercontent.com/GaiaSaveri/intro-to-ml/main/data/Mushroom.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1265101 (1,2M) [text/plain]\n",
      "Saving to: ‘Mushroom.csv’\n",
      "\n",
      "Mushroom.csv        100%[===================>]   1,21M  --.-KB/s    in 0,09s   \n",
      "\n",
      "2024-11-12 15:52:32 (13,2 MB/s) - ‘Mushroom.csv’ saved [1265101/1265101]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FFILE = './Mushrooms.csv'\n",
    "if os.path.isfile(FFILE):\n",
    "    print(\"File already exists\")\n",
    "    if os.access(FFILE, os.R_OK):\n",
    "        print (\"File is readable\")\n",
    "    else:\n",
    "        print (\"File is not readable, removing it and downloading again\")\n",
    "        !rm FFILE\n",
    "        !wget \"https://raw.githubusercontent.com/GaiaSaveri/intro-to-ml/main/data/Mushroom.csv\"\n",
    "else:\n",
    "    print(\"Either the file is missing or not readable, download it\")\n",
    "    !wget \"https://raw.githubusercontent.com/GaiaSaveri/intro-to-ml/main/data/Mushroom.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trKzFVZ2aFVp"
   },
   "source": [
    "Divide features and labels and split the dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "rkF6XEd1aU22",
    "outputId": "74dcca7b-b531-4f00-9cc1-821fa42e44b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>CAP-SHAPE</th>\n",
       "      <th>CAP-SURF</th>\n",
       "      <th>CAP-COLOR</th>\n",
       "      <th>BRUISES</th>\n",
       "      <th>ODOR</th>\n",
       "      <th>GILL-ATTACH</th>\n",
       "      <th>GILL-SPACE</th>\n",
       "      <th>GILL-SIZE</th>\n",
       "      <th>GILL-COLOR</th>\n",
       "      <th>...</th>\n",
       "      <th>STALK-SURF-BELOW</th>\n",
       "      <th>STALK-COLOR-ABOVE</th>\n",
       "      <th>STALK-COLOR-BELOW</th>\n",
       "      <th>VEIL-TYPE</th>\n",
       "      <th>VEIL-COLOR</th>\n",
       "      <th>RING-NUM</th>\n",
       "      <th>RING-TYPE</th>\n",
       "      <th>SPORE-PRINT-COLOR</th>\n",
       "      <th>POP</th>\n",
       "      <th>HABIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EDIBLE</td>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRUISES</td>\n",
       "      <td>ALMOND</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CROWDED</td>\n",
       "      <td>NARROW</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>PURPLE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EDIBLE</td>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRUISES</td>\n",
       "      <td>ALMOND</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CROWDED</td>\n",
       "      <td>NARROW</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EDIBLE</td>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRUISES</td>\n",
       "      <td>ALMOND</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CROWDED</td>\n",
       "      <td>NARROW</td>\n",
       "      <td>PINK</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>PURPLE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDIBLE</td>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRUISES</td>\n",
       "      <td>ALMOND</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CROWDED</td>\n",
       "      <td>NARROW</td>\n",
       "      <td>PINK</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDIBLE</td>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRUISES</td>\n",
       "      <td>ALMOND</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CROWDED</td>\n",
       "      <td>NARROW</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>PURPLE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CLASS CAP-SHAPE CAP-SURF CAP-COLOR  BRUISES    ODOR GILL-ATTACH  \\\n",
       "0  EDIBLE    CONVEX   SMOOTH     WHITE  BRUISES  ALMOND        FREE   \n",
       "1  EDIBLE    CONVEX   SMOOTH     WHITE  BRUISES  ALMOND        FREE   \n",
       "2  EDIBLE    CONVEX   SMOOTH     WHITE  BRUISES  ALMOND        FREE   \n",
       "3  EDIBLE    CONVEX   SMOOTH     WHITE  BRUISES  ALMOND        FREE   \n",
       "4  EDIBLE    CONVEX   SMOOTH     WHITE  BRUISES  ALMOND        FREE   \n",
       "\n",
       "  GILL-SPACE GILL-SIZE GILL-COLOR  ... STALK-SURF-BELOW STALK-COLOR-ABOVE  \\\n",
       "0    CROWDED    NARROW      WHITE  ...           SMOOTH             WHITE   \n",
       "1    CROWDED    NARROW      WHITE  ...           SMOOTH             WHITE   \n",
       "2    CROWDED    NARROW       PINK  ...           SMOOTH             WHITE   \n",
       "3    CROWDED    NARROW       PINK  ...           SMOOTH             WHITE   \n",
       "4    CROWDED    NARROW      BROWN  ...           SMOOTH             WHITE   \n",
       "\n",
       "  STALK-COLOR-BELOW VEIL-TYPE VEIL-COLOR RING-NUM RING-TYPE SPORE-PRINT-COLOR  \\\n",
       "0             WHITE   PARTIAL      WHITE      ONE   PENDANT            PURPLE   \n",
       "1             WHITE   PARTIAL      WHITE      ONE   PENDANT             BROWN   \n",
       "2             WHITE   PARTIAL      WHITE      ONE   PENDANT            PURPLE   \n",
       "3             WHITE   PARTIAL      WHITE      ONE   PENDANT             BROWN   \n",
       "4             WHITE   PARTIAL      WHITE      ONE   PENDANT            PURPLE   \n",
       "\n",
       "       POP  HABIT  \n",
       "0  SEVERAL  WOODS  \n",
       "1  SEVERAL  WOODS  \n",
       "2  SEVERAL  WOODS  \n",
       "3  SEVERAL  WOODS  \n",
       "4  SEVERAL  WOODS  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_m=pd.read_csv(\"./Mushroom.csv\")\n",
    "train_data_m.head()  # viewing some row of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AfO2CdaZHzxP",
    "outputId": "0a62e56f-01bc-43cf-f1b4-b9a5fec34875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8416, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "0UWaVxlfUQzL",
    "outputId": "cfd86c31-00d4-4ee6-a45d-380f9d7da8f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>CAP-SHAPE</th>\n",
       "      <th>CAP-SURF</th>\n",
       "      <th>CAP-COLOR</th>\n",
       "      <th>BRUISES</th>\n",
       "      <th>ODOR</th>\n",
       "      <th>GILL-ATTACH</th>\n",
       "      <th>GILL-SPACE</th>\n",
       "      <th>GILL-SIZE</th>\n",
       "      <th>GILL-COLOR</th>\n",
       "      <th>...</th>\n",
       "      <th>STALK-SURF-BELOW</th>\n",
       "      <th>STALK-COLOR-ABOVE</th>\n",
       "      <th>STALK-COLOR-BELOW</th>\n",
       "      <th>VEIL-TYPE</th>\n",
       "      <th>VEIL-COLOR</th>\n",
       "      <th>RING-NUM</th>\n",
       "      <th>RING-TYPE</th>\n",
       "      <th>SPORE-PRINT-COLOR</th>\n",
       "      <th>POP</th>\n",
       "      <th>HABIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POISONOUS</td>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>NO</td>\n",
       "      <td>SPICY</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>NARROW</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EDIBLE</td>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>RED</td>\n",
       "      <td>BRUISES</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>BROAD</td>\n",
       "      <td>PURPLE</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>PINK</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POISONOUS</td>\n",
       "      <td>FLAT</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>NO</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>BROAD</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>...</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>LARGE</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>SOLITARY</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDIBLE</td>\n",
       "      <td>FLAT</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CROWDED</td>\n",
       "      <td>BROAD</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>ABUNDANT</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDIBLE</td>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>BRUISES</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FREE</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>BROAD</td>\n",
       "      <td>PURPLE</td>\n",
       "      <td>...</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ONE</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CLASS CAP-SHAPE CAP-SURF CAP-COLOR  BRUISES   ODOR GILL-ATTACH  \\\n",
       "0  POISONOUS    CONVEX    SCALY     BROWN       NO  SPICY        FREE   \n",
       "1     EDIBLE    CONVEX  FIBROUS       RED  BRUISES   NONE        FREE   \n",
       "2  POISONOUS      FLAT  FIBROUS      GRAY       NO   FOUL        FREE   \n",
       "3     EDIBLE      FLAT  FIBROUS     BROWN       NO   NONE        FREE   \n",
       "4     EDIBLE    CONVEX  FIBROUS     BROWN  BRUISES   NONE        FREE   \n",
       "\n",
       "  GILL-SPACE GILL-SIZE GILL-COLOR  ... STALK-SURF-BELOW STALK-COLOR-ABOVE  \\\n",
       "0      CLOSE    NARROW       BUFF  ...           SMOOTH             WHITE   \n",
       "1      CLOSE     BROAD     PURPLE  ...           SMOOTH              PINK   \n",
       "2      CLOSE     BROAD  CHOCOLATE  ...            SILKY              BUFF   \n",
       "3    CROWDED     BROAD      BROWN  ...           SMOOTH             WHITE   \n",
       "4      CLOSE     BROAD     PURPLE  ...           SMOOTH              GRAY   \n",
       "\n",
       "  STALK-COLOR-BELOW VEIL-TYPE VEIL-COLOR RING-NUM   RING-TYPE  \\\n",
       "0              PINK   PARTIAL      WHITE      ONE  EVANESCENT   \n",
       "1             WHITE   PARTIAL      WHITE      ONE     PENDANT   \n",
       "2              PINK   PARTIAL      WHITE      ONE       LARGE   \n",
       "3             WHITE   PARTIAL      WHITE      ONE  EVANESCENT   \n",
       "4             WHITE   PARTIAL      WHITE      ONE     PENDANT   \n",
       "\n",
       "  SPORE-PRINT-COLOR       POP    HABIT  \n",
       "0             WHITE   SEVERAL    WOODS  \n",
       "1             BROWN   SEVERAL    WOODS  \n",
       "2         CHOCOLATE  SOLITARY  GRASSES  \n",
       "3             BROWN  ABUNDANT  GRASSES  \n",
       "4             BROWN   SEVERAL    WOODS  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use first 5000 data points for training and the rest for test\n",
    "train_data_m = train_data_m.sample(frac=1,random_state=0).reset_index(drop=True) # random shufle\n",
    "train_data_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OIQJ-4q6Uk-o"
   },
   "outputs": [],
   "source": [
    "# Create a training set 'train' by selecting the first 5000 rows of the DataFrame 'train_data_m'\n",
    "train = train_data_m.iloc[:5000, :]\n",
    "\n",
    "# Create a test set 'test' by selecting rows from position 5000 onwards until the end of 'train_data_m'\n",
    "test = train_data_m.iloc[5000:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tvWNi6MKb_D"
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "**Using Decision Trees for Classification with ID3 (Iterative Dichotomiser 3)**\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "- **Decision Building:**\n",
    "  - We build a decision tree from a dataset.\n",
    "  - Each node in the tree is like a checkpoint, helping us make a decision (internal node) or representing an outcome (leaves).\n",
    "\n",
    "- **ID3 Algorithm:**\n",
    "  - ID3 is a top-down approach (starting from the root) and a greedy algorithm (only considers the current step when choosing the best features) for building decision trees.\n",
    "\n",
    "- **Feature Selection:**\n",
    "  - At each step, features are grouped based on **information gain**.\n",
    "  - Information gain helps us decide which feature is the most useful for making decisions.\n",
    "  - **Entropy Formula:** $H(S) = \\sum_{i=1}^D -p_i \\log p_i$, where $p_i$ is the proportion of each category.\n",
    "  - **Information Gain Formula:** $IG(S, j)=H(s) - \\sum_j \\frac{|S_j|}{|S|}H(S_j)$\n",
    "\n",
    "- **Node Selection:**\n",
    "  - A node becomes a leaf if all the data in that node belong to the same class.\n",
    "\n",
    "- **Tree Construction:**\n",
    "  - Repeat this process until the tree has only leaf nodes or until we've used all available features.\n",
    "\n",
    "We start at the top and make decisions at each step based on the most helpful information. This continues until all our decisions (leaves) are clear or until we've used up all the information we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncpERyjffrwQ"
   },
   "source": [
    "Functions for building the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UH7kMkSOZKCp"
   },
   "outputs": [],
   "source": [
    "# compute H(S)\n",
    "def calc_total_entropy(train_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : matrix n_data x n_features\n",
    "        Matrix containing the training dataset\n",
    "    label : int\n",
    "        Feature used as label\n",
    "    class_list : list of str\n",
    "        Possible values of the label\n",
    "    \"\"\"\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gu--JA60XM03"
   },
   "outputs": [],
   "source": [
    "# compute H(S_j)\n",
    "def calc_entropy(feature_value_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_value_data : matrix n_data_selected x n_features\n",
    "        Matrix containing the training points having a certain value of feature j\n",
    "    label : int\n",
    "        Feature used as label\n",
    "    class_list : list of str\n",
    "        Possible values of the label\n",
    "    \"\"\"\n",
    "    class_count = feature_value_data.shape[0] # Number of points considered\n",
    "    entropy = 0\n",
    "\n",
    "    for c in class_list:  # For each possible class in the label\n",
    "        label_class_count = feature_value_data[feature_value_data[label] == c].shape[0]  # Count of rows with class c\n",
    "        entropy_class = 0\n",
    "        if label_class_count != 0:  # Avoid numerical errors\n",
    "            probability_class = label_class_count / class_count  # Probability of the class\n",
    "            entropy_class = - probability_class * np.log2(probability_class)  # Entropy\n",
    "        entropy += entropy_class\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjc4K3p6MnJk"
   },
   "source": [
    "The function returns the entropy of the subset based on the specified feature value and its possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpXNL4Z-ZHMO"
   },
   "outputs": [],
   "source": [
    "# compute information gain in terms of entropy IG(S, j)\n",
    "def calc_info_gain(feature_name, train_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_name : str\n",
    "        Feature considered for computing information gain (j)\n",
    "    train_data : matrix n_data x n_features\n",
    "        Matrix containing the training dataset\n",
    "    label : int\n",
    "        Feature used as label\n",
    "    class_list : list of str\n",
    "        Possible values of the label\n",
    "    \"\"\"\n",
    "    feature_value_list = train_data[feature_name].unique() # Unique values of the feature\n",
    "    total_row = train_data.shape[0]\n",
    "    feature_info = 0.0\n",
    "\n",
    "    ##### Check for missing values\n",
    "    a = feature_value_list[feature_value_list=='?']\n",
    "    t = a.shape[0] # Number of points with missing value\n",
    "\n",
    "    if t > 0:  # At least a point is missing the entry for this feature\n",
    "        cmax = 0  # Number of points in the most represented class\n",
    "        n = -1\n",
    "        for feature_value in feature_value_list:  # Possible values for the current feature\n",
    "            n = n + 1\n",
    "            if feature_value != '?':\n",
    "                c = train_data[train_data[feature_name] == feature_value].shape[0]\n",
    "                if c > cmax:\n",
    "                    cmax = c\n",
    "                    fmax = feature_value  # Value of the feature with the most points\n",
    "        # Replace missing values with the most represented feature value\n",
    "        train_data[feature_name][:] = [fmax if x == '?' else x for x in train_data[feature_name]]\n",
    "\n",
    "    #####  Now all the data have a value for the feature under analysis\n",
    "    for feature_value in feature_value_list:\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value]  # Filtering rows with that feature_value\n",
    "        feature_value_count = feature_value_data.shape[0]  # Number of points having feature value in feature j\n",
    "        feature_value_entropy = calc_entropy(feature_value_data, label, class_list) # Calculating entropy for the feature value\n",
    "        feature_value_probability = feature_value_count / total_row\n",
    "        feature_info += feature_value_probability * feature_value_entropy  # Calculating information of the feature value\n",
    "\n",
    "    return calc_total_entropy(train_data, label, class_list) - feature_info  # Calculating information gain by subtracting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBFMAKdpNd7P"
   },
   "source": [
    "- **Handling Missing Values:**\n",
    "  - Check for missing values in the feature, represented by `'?'`.\n",
    "  - If there are missing values, find the most represented feature value (`fmax`) and replace missing values with it.\n",
    "\n",
    "- **Information Gain Calculation Loop:**\n",
    "  - For each unique value of the feature:\n",
    "    - `feature_value_data`: Filter rows with the current feature value.\n",
    "    - `feature_value_count`: Number of points having the current feature value.\n",
    "    - `feature_value_entropy`: Calculate the entropy for the current feature value.\n",
    "    - `feature_value_probability`: Probability of the current feature value occurring.\n",
    "    - Add the product of probability and entropy to the total information of the feature (`feature_info`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LDMVcNKbjR-"
   },
   "outputs": [],
   "source": [
    "# find feature with maximum information gain\n",
    "def find_most_informative_feature(train_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : matrix n_data x n_features\n",
    "        Matrix containing the training dataset\n",
    "    label : int\n",
    "        Feature used as label\n",
    "    class_list : list of str\n",
    "        Possible values of the label\n",
    "    \"\"\"\n",
    "    # N.B. label is not a feature, so drop it!\n",
    "    feature_list = train_data.columns.drop(label) #finding the feature names in the dataset\n",
    "    max_info_gain = -1\n",
    "    max_info_feature = None\n",
    "\n",
    "    for feature in feature_list:  # for each feature in the dataset\n",
    "        feature_info_gain = calc_info_gain(feature, train_data, label, class_list)\n",
    "        if max_info_gain < feature_info_gain: # selecting feature name with highest information gain\n",
    "            max_info_gain = feature_info_gain\n",
    "            max_info_feature = feature\n",
    "\n",
    "    return max_info_feature  # returns the name of the feature with the maximum information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0F1Klrnb4xU"
   },
   "outputs": [],
   "source": [
    "# split the tree and check finishing condition\n",
    "def generate_sub_tree(feature_name, train_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_name : str\n",
    "        Feature considered at current node\n",
    "    train_data : matrix n_data x n_features\n",
    "        Matrix containing the training dataset\n",
    "    label : int\n",
    "        Feature used as label\n",
    "    class_list : list of str\n",
    "        Possible values of the label\n",
    "    \"\"\"\n",
    "    feature_value_count_dict = train_data[feature_name].value_counts(sort=False)  # dictionary of the count of unqiue feature value\n",
    "    tree = {}  # sub tree or node\n",
    "\n",
    "    for feature_value, count in feature_value_count_dict.items():\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value]  # dataset with only feature_name = feature_value\n",
    "        assigned_to_node = False  # flag for tracking feature_value is pure class or not\n",
    "        for c in class_list:  # for each class\n",
    "            class_count = feature_value_data[feature_value_data[label] == c].shape[0]  # count of class c\n",
    "\n",
    "            if class_count == count:  # count of (feature_value = count) of class (pure class, any value of the feature represents only one class)\n",
    "                tree[feature_value] = c  # adding node to the tree\n",
    "                train_data = train_data[train_data[feature_name] != feature_value]  # removing rows with feature_value\n",
    "                assigned_to_node = True\n",
    "        if not assigned_to_node:  # not pure class\n",
    "            tree[feature_value] = \"?\" # as feature_value is not a pure class, it should be expanded further,\n",
    "                                      # so the branch is marking with ?\n",
    "\n",
    "    return tree, train_data # The function returns the generated subtree (`tree`) and the updated dataset (`train_data`) after removing rows associated with the feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRpY0OgFPsWJ"
   },
   "source": [
    "- **Subtree Generation Loop:**\n",
    "  - For each unique feature value and its count:\n",
    "    - `feature_value_data`: The subset of the dataset with only the current feature value.\n",
    "    - `assigned_to_node`: A flag to track whether the feature value represents a pure class.\n",
    "    - For each class in the class list:\n",
    "      - `class_count`: The count of the current class in the subset.\n",
    "      - If the count of the class is equal to the count of the feature value, it's a pure class, and a node is added to the tree.\n",
    "    - If the feature value is not assigned to a node (not a pure class), a branch is marked with \"?\" in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSqldicId08d"
   },
   "outputs": [],
   "source": [
    "# generate the tree\n",
    "def make_tree(root, prev_feature_value, train_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : dict\n",
    "        Tree written as dictionary of subtrees (initially {})\n",
    "    prev_feature_value : str\n",
    "        Previous value of the pointed node (initially None)\n",
    "    train_data : matrix n_data x n_features\n",
    "        Matrix containing the training dataset\n",
    "    label : int\n",
    "        Feature used as label\n",
    "    class_list : list of str\n",
    "        Possible values of the label\n",
    "    \"\"\"\n",
    "    if train_data.shape[0] != 0:  # if dataset becomes empty after updating\n",
    "        max_info_feature = find_most_informative_feature(train_data, label, class_list)  # most informative feature\n",
    "        tree, train_data = generate_sub_tree(max_info_feature, train_data, label, class_list)  # getting tree node and updated dataset\n",
    "        next_root = None\n",
    "        if prev_feature_value != None:  # add to intermediate node of the tree\n",
    "            root[prev_feature_value] = dict()\n",
    "            root[prev_feature_value][max_info_feature] = tree  # expand the tree\n",
    "            next_root = root[prev_feature_value][max_info_feature]\n",
    "        else:  # add to root of the tree\n",
    "            root[max_info_feature] = tree\n",
    "            next_root = root[max_info_feature]\n",
    "\n",
    "        for node, branch in list(next_root.items()):  # iterating the tree node\n",
    "            if branch == \"?\":  # if it is expandable\n",
    "                feature_value_data = train_data[train_data[max_info_feature] == node]  # using the updated dataset\n",
    "                make_tree(next_root, node, feature_value_data, label, class_list)  # recursive call with updated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHzqej26QIES"
   },
   "source": [
    "- **Tree Generation Steps:**\n",
    "  - `max_info_feature`: Identifies the most informative feature for the current node.\n",
    "  - `tree, train_data`: Generates a subtree for the identified feature using the `generate_sub_tree` function and updates the dataset.\n",
    "  - `next_root`: The subtree corresponding to the current node.\n",
    "\n",
    "- **Adding to Tree:**\n",
    "  - If `prev_feature_value` is not `None`, it adds the subtree to the intermediate node of the tree.\n",
    "  - If `prev_feature_value` is `None`, it adds the subtree to the root of the tree.\n",
    "\n",
    "- **Recursive Call:**\n",
    "  - For each node and branch in the current subtree, if the branch is \"?\" (expandable), it recursively calls the function with the updated dataset. This step continues until the dataset becomes empty or all branches are non-expandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBvUWsfGfFXI"
   },
   "outputs": [],
   "source": [
    "# id3 call\n",
    "def id3(train_data_m, label):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data_m : matrix n_data x n_features\n",
    "        Matrix containing the training dataset\n",
    "    label : int\n",
    "        Feature used as label\n",
    "    \"\"\"\n",
    "    train_data = train_data_m.copy()  # getting a copy of the dataset\n",
    "    tree = {}  # tree which will be updated\n",
    "    class_list = train_data[label].unique()  # getting unqiue classes of the label\n",
    "    make_tree(tree, None, train_data, label, class_list)  # start calling recursion\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8y1UudLS3qV"
   },
   "source": [
    "- **`id3` Function:**\n",
    "  - This function is the entry point for the ID3 algorithm. It initializes the tree and calls the `make_tree` function to generate the decision tree recursively.\n",
    "\n",
    "- **Initialization:**\n",
    "  - `train_data`: A copy of the original dataset. The copy is made to avoid modifying the original dataset during the tree generation process.\n",
    "  - `tree`: The variable to store the decision tree, initialized as an empty dictionary.\n",
    "  - `class_list`: A list of unique classes present in the label.\n",
    "\n",
    "- **Recursive Tree Generation:**\n",
    "  - Calls the `make_tree` function with the initial parameters to start the recursive tree generation process. The tree is updated during this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVJS1vcNfkMy"
   },
   "source": [
    "Functions for testing the decision tree algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaJHf4jkffmN"
   },
   "outputs": [],
   "source": [
    "# prediction from a given instance\n",
    "def predict(tree, instance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : dict\n",
    "        Decision tree represented as a dictionary\n",
    "    instance : dict\n",
    "        Instance to be classified\n",
    "    \"\"\"\n",
    "    # TODO: Handle missing values or missing label in the training set\n",
    "    if not isinstance(tree, dict):  # If it is a leaf node\n",
    "        return tree  # Return the value\n",
    "    else:\n",
    "        root_node = next(iter(tree))  # Getting the first key/feature name of the dictionary\n",
    "        feature_value = instance[root_node]  # Value of the feature\n",
    "        if feature_value in tree[root_node]:  # Checking if the feature value is in the current tree node\n",
    "            if (feature_value != \"?\"):  # & (feature_value in tree.keys())):\n",
    "                return predict(tree[root_node][feature_value], instance)  # Go to the next feature\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G9rpqEWTfx0"
   },
   "source": [
    "- **Leaf Node Check:**\n",
    "  - If the current node in the decision tree is not a dictionary (i.e., a leaf node), it returns the value associated with that node.\n",
    "\n",
    "- **Non-Leaf Node Processing:**\n",
    "  - If the current node is a dictionary (non-leaf node), it extracts the feature name of the current node (`root_node`) and the corresponding feature value from the instance.\n",
    "\n",
    "- **Checking Feature Value in Tree:**\n",
    "  - It checks if the feature value exists in the current tree node. If it does, and the feature value is not \"?\" (indicating a missing value), it recursively calls the `predict` function for the next level of the tree.\n",
    "\n",
    "- **Handling Unknown Feature Values:**\n",
    "  - If the feature value is not found in the current tree node, it returns `None`, indicating that the tree does not provide a prediction for the given instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExsshgdMffv1"
   },
   "outputs": [],
   "source": [
    "# accuracy evaluation\n",
    "def evaluate(tree, test_data_m, label):\n",
    "    correct_preditct = 0\n",
    "    wrong_preditct = 0\n",
    "    for index in range(len(test_data_m.index)):  # for each row in the dataset\n",
    "        result = predict(tree, test_data_m.iloc[index])  # predict the row\n",
    "        if result == test_data_m[label].iloc[index]:  # predicted value and expected value is same or not\n",
    "            correct_preditct += 1  # increase correct count\n",
    "        else:\n",
    "            wrong_preditct += 1  # increase incorrect count\n",
    "    accuracy = correct_preditct / (correct_preditct + wrong_preditct)  # calculating accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mo9W9qaZKVH"
   },
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Make classification using the **Naive Bayes** algorithm.\n",
    "\n",
    "Recall:\n",
    "\n",
    "* Bayes Theorem: $p(class|data) \\propto p(data|class)\\cdot p(class)$\n",
    "\n",
    "This theorem expresses how our belief in the probability of a particular class given some data is proportional to the likelihood of observing that data given the class, multiplied by the prior probability of the class. In other words, it helps us update our beliefs about the probability of a class based on new evidence\n",
    "\n",
    "* prior ($p(class)$) is just the ratio of the number of datapoints belonging to the class;\n",
    "\n",
    "The prior probability is simply the ratio of the number of data points belonging to a specific class to the total number of data points. It represents our initial belief in the likelihood of encountering a particular class before considering any additional information.\n",
    "\n",
    "* to make predictions (i.e. compute the posterior $p(class|data)$) we consider the likelihood of each class ($p(data|class)$) computed as a proportion;\n",
    "\n",
    "* we work in the log-space so that predictions will be the class maximizing the sum of prior and likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Njx_cXjgbom"
   },
   "source": [
    "Function for training the Naive Bayes algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ets-ledrcTIL"
   },
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_data, label):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : matrix n_data x n_features\n",
    "        Matrix containing the training dataset\n",
    "    label : int\n",
    "        Feature used as label\n",
    "    \"\"\"\n",
    "    bayes_pi = {}\n",
    "    bayes_tab = {}\n",
    "    ntot = len(train_data.index)\n",
    "    for cl in train_data[label].unique():  # for each possible value of the label\n",
    "        fl = train_data[label]  # select training points in current class\n",
    "        ncl = fl[fl==cl].shape[0]  # count number of points in current class\n",
    "        pcl = ncl/ntot # proportion of points in current class (prior)\n",
    "        bayes_pi[cl] = pcl\n",
    "    for col in train_data.columns: # for each feature\n",
    "        if (col != label):\n",
    "            dd = pd.crosstab(train_data[label], train_data[col]) # frequency table\n",
    "            a = np.sum(dd)  # total number of points belonging to a class\n",
    "            b = np.sum(a[a.keys()!=\"?\"])\n",
    "            bayes_tab[col] = dd/b # likelihhod of each class\n",
    "\n",
    "    return bayes_pi, bayes_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtKNvmhBjK5W"
   },
   "source": [
    "Function for testing Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaS8VGpggfaU"
   },
   "outputs": [],
   "source": [
    "# prediction and accuracy evaluation\n",
    "def predict_naive_bayes(test_data, bayes_pi, bayes_tab, label):\n",
    "    ntot = len(test_data.index)\n",
    "    ncorrect = 0\n",
    "    for j in range(ntot):\n",
    "        prob = bayes_pi.copy()\n",
    "        for col in test_data.columns:\n",
    "            if (col != label):\n",
    "                if ((test_data[col].iloc[j]!=\"?\")&(test_data[col].iloc[j] in bayes_tab[col].keys())):\n",
    "                    for cl in bayes_pi.keys():\n",
    "                        prob[cl] = prob[cl]*bayes_tab[col][test_data[col].iloc[j]][cl]\n",
    "        if (test_data[label].iloc[j] == max(prob, key=prob.get)):\n",
    "            ncorrect = ncorrect + 1\n",
    "    return (ncorrect/ntot) # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8tLyXsflNoZ"
   },
   "source": [
    "## Results on Mushroom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiPr8tOYlYEi"
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4BPTgeXlMxu",
    "outputId": "85b0cc2f-5142-4e4f-b355-39209f8e3757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ODOR': {'ALMOND': 'EDIBLE', 'ANISE': 'EDIBLE', 'NONE': 'EDIBLE', 'PUNGENT': 'POISONOUS', 'CREOSOTE': 'POISONOUS', 'FOUL': 'POISONOUS'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-d06204b963d6>:18: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n"
     ]
    }
   ],
   "source": [
    "tree = id3(train, \"CLASS\")\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpu1eag3loSF",
    "outputId": "f5862dc3-df3d-4740-ec2b-d6fdfbb380e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6135831381733021\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(tree, test, \"CLASS\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYZm1lw4la5D"
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxbQUc7KlnIn"
   },
   "outputs": [],
   "source": [
    "pi, tab = train_naive_bayes(train, \"CLASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gsm-i5VdmYBE",
    "outputId": "5390f8f9-395a-4566-8a32-2e95c3910641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36065573770491804\n"
     ]
    }
   ],
   "source": [
    "accuracy = predict_naive_bayes(test, pi, tab,\"CLASS\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaRXsv9UXBNC"
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "\n",
    "\n",
    "*  [ Decision and Classification Trees: Explained](https://youtu.be/_L39rN6gz7Y?si=nNZVbaBuq28pPXe4)\n",
    "*   [Entropy (for data science) Explained](https://youtu.be/YtebGVx-Fxw?si=2iDmdSdJgyZ7gVYa)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
