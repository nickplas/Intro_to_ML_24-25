{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc23f7f3",
   "metadata": {
    "id": "dc23f7f3"
   },
   "source": [
    "# Implicit Regularization\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/nickplas/Intro_to_ML_24-25/blob/main/notebooks/Lab-12.Implicit_regularization.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In this lab, we will study the *implicit bias* induced by *Gradient Descent* optimization in the simple case of *linear regression*, fitted on a toy dataset. In particular, we will show that GD-optimized weights converge to the **least norm** solution of the linear regression problem.\n",
    "\n",
    "Then, we will study how different initializations and different optimizers affect the weights learned by a *Convolutional neural network*.\n",
    "\n",
    "\n",
    "An analysis of implicit regularization (aka implicit bias) induced by *Stochastic Gradient Descent* in *full-width linear fully-connected* and *full-width linear convolutional* neural networks is provided in [this paper](https://arxiv.org/abs/1806.00468).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91a3f4",
   "metadata": {
    "id": "7f91a3f4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg as LA\n",
    "import random\n",
    "\n",
    "random.seed(546)\n",
    "np.random.seed(987)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dnrReSSNIk86",
   "metadata": {
    "id": "dnrReSSNIk86"
   },
   "source": [
    "## Linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OBNgdeBFJfxC",
   "metadata": {
    "id": "OBNgdeBFJfxC"
   },
   "source": [
    "Let's first create a function which generates data. You've already seen this in many previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e67ec4",
   "metadata": {
    "id": "37e67ec4"
   },
   "outputs": [],
   "source": [
    "def datagen(d, points, m, M, w, sigma):\n",
    "    X = np.zeros((points,d))\n",
    "    for i in range(points):\n",
    "        X[i,:] = np.random.uniform(m, M, d)\n",
    "    eps = np.random.normal(0, sigma, points)\n",
    "    y = np.dot(X,w) + eps \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1O5SWsZzJqTy",
   "metadata": {
    "id": "1O5SWsZzJqTy"
   },
   "source": [
    "We can visualise the data created if $d=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7e284",
   "metadata": {
    "id": "27f7e284"
   },
   "outputs": [],
   "source": [
    "d = 1\n",
    "w = np.random.normal(0, 1, d)\n",
    "sigma = 3\n",
    "points = 100\n",
    "m = -10\n",
    "M = 10\n",
    "\n",
    "X, y = datagen(d, points, m, M, w, sigma)\n",
    "fig, ax =plt.subplots()\n",
    "ax.scatter(X,y)\n",
    "ax.plot(X,np.dot(X,w), color = \"red\")\n",
    "ax.set_title('Data')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cJmwGuJ4gp",
   "metadata": {
    "id": "f3cJmwGuJ4gp"
   },
   "source": [
    "Let's now create the data set that we will use, in a higher dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8MrMWn0JajR",
   "metadata": {
    "id": "a8MrMWn0JajR"
   },
   "outputs": [],
   "source": [
    "d = 10\n",
    "w = np.random.normal(0,1,d)\n",
    "sigma = 3\n",
    "points = 1000\n",
    "m = -10\n",
    "M = 10\n",
    "\n",
    "X, y = datagen(d, points, m, M, w, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HR9TVLIEUfiA",
   "metadata": {
    "id": "HR9TVLIEUfiA"
   },
   "source": [
    "In the case of **linear regression**, fitted by means of *least squares*, we optimize the following loss function:\n",
    "$$\n",
    "L=\\|y-Xw\\|_{2}^{2}.\n",
    "$$\n",
    "\n",
    "If we choose the *GD* optimization algorithm, we perform weight updates proportional to the gradient of the loss function:\n",
    "$$\n",
    "\\nabla_{w} L = -X(y-Xw).\n",
    "$$\n",
    "\n",
    "Additionally, notice that the **least norm** solution of the *linear regression* problem is given by:\n",
    "$$\n",
    "w^{*}=(X^{T}X + \\lambda I)^{-1}X^{T}y.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FqHXgO-TFf4O",
   "metadata": {
    "id": "FqHXgO-TFf4O"
   },
   "source": [
    "##### 1. Compute the least norm solution of the linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DwUVqSemFqzE",
   "metadata": {
    "id": "DwUVqSemFqzE"
   },
   "outputs": [],
   "source": [
    "def least_norm_reg(X, y, lam):\n",
    "    # YOUR CODE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6qN7OsqXGfLK",
   "metadata": {
    "id": "6qN7OsqXGfLK"
   },
   "source": [
    "##### 2. Perform GD optimization of the linear regression problem iteratively, storing the weights at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zTsrBIr5HGVy",
   "metadata": {
    "id": "zTsrBIr5HGVy"
   },
   "outputs": [],
   "source": [
    "# gradient of the MSE corresponding to the ordinary least squares solution for linear regression\n",
    "def OLSGradient(X, y, w):\n",
    "    # YOUR CODE\n",
    "    pass\n",
    "\n",
    "# gradient descent for linear regression with learning rate gamma\n",
    "def GD(X, y, Iter, gamma):\n",
    "    # YOUR CODE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Usgf8T43HosN",
   "metadata": {
    "id": "Usgf8T43HosN"
   },
   "source": [
    "##### 3. Plot the evolution of the weights during GD optimization, showing their relation with the least norm solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JIxQK2-7JHEr",
   "metadata": {
    "id": "JIxQK2-7JHEr"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JYNvh7QOolNS",
   "metadata": {
    "id": "JYNvh7QOolNS"
   },
   "source": [
    "## Fully Connected Neural Network\n",
    "\n",
    "*Remember to enable GPU acceleration! `Runtime` > `Change runtime type` > `Hardware acceleration` > `GPU`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VOq2SNzlrwjy",
   "metadata": {
    "id": "VOq2SNzlrwjy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ZavONBp3wFu",
   "metadata": {
    "id": "7ZavONBp3wFu"
   },
   "source": [
    "### Loading the data set\n",
    "\n",
    "The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) data set consists of $60000$ $32x32$ colour images in $10$ classes, with $6000$ images per class. There are $50000$ training images and $10000$ test images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CbX-k_qx3VWO",
   "metadata": {
    "id": "CbX-k_qx3VWO"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Convert data to PyTorch tensor form and normalise it\n",
    "# The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].\n",
    "data_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "## download and load training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fNLade_U46Jr",
   "metadata": {
    "id": "fNLade_U46Jr"
   },
   "source": [
    "#### Visualising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zDbQ8nJX3oHN",
   "metadata": {
    "id": "zDbQ8nJX3oHN"
   },
   "outputs": [],
   "source": [
    "num_row = 2\n",
    "num_col = 5\n",
    "\n",
    "classes = trainset.classes\n",
    "\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col, 2*num_row))\n",
    "for i in range(num_row*num_col):\n",
    "    ax = axes[i//num_col, i%num_col]\n",
    "    ax.imshow(trainset.data[i], cmap='gray')\n",
    "    ax.set_title('Label: {}'.format(classes[trainset.targets[i]]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DpKETKQ543WG",
   "metadata": {
    "id": "DpKETKQ543WG"
   },
   "outputs": [],
   "source": [
    "# printing data shapes \n",
    "print(\"Single image dimensions:\", trainset.data[0].shape)\n",
    "for images, labels in trainloader:\n",
    "    print(\"Image batch dimensions:\", images.shape)\n",
    "    print(\"Image label dimensions:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YGn6YOz-5Cr8",
   "metadata": {
    "id": "YGn6YOz-5Cr8"
   },
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TBCGu5WWWPYz",
   "metadata": {
    "id": "TBCGu5WWWPYz"
   },
   "source": [
    "##### 4. Define a CNN with one convolutional layer and two linear layers (be careful of the dimensions!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F51zAU5_r-6v",
   "metadata": {
    "id": "F51zAU5_r-6v"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    # YOUR CODE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GXS25ZnO5Iom",
   "metadata": {
    "id": "GXS25ZnO5Iom"
   },
   "source": [
    "#### Defining the auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EV7So8aLtHyX",
   "metadata": {
    "id": "EV7So8aLtHyX"
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def get_accuracy(logit, target):\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def compute_weight_norm(model):\n",
    "    norm = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "      if \"weight\" in name:\n",
    "        norm += torch.norm(param.data, p= 2)\n",
    "    return norm.cpu().item()\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs, trainloader, criterion, optimizer):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    norms = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_running_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        # Set the model to training mode\n",
    "        model = model.train()\n",
    "        start = time()\n",
    "        ## training step\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            ## forward + backprop + loss\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Reset the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            ## update model params\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss += loss.item()\n",
    "            train_acc += get_accuracy(logits, labels)\n",
    "        \n",
    "            \n",
    "        losses.append(train_running_loss / i)\n",
    "        accs.append(train_acc/i)\n",
    "        \n",
    "        norms.append(compute_weight_norm(model))\n",
    "        model.eval()\n",
    "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss / i:.4f} | Train Accuracy: {train_acc/i:.4f} | Time: {time()-start:.2f}\") \n",
    "    \n",
    "    return losses, accs, norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1qPW_bfo5WbY",
   "metadata": {
    "id": "1qPW_bfo5WbY"
   },
   "source": [
    "#### Training with different optimizers\n",
    "\n",
    "An optimizer is an algorithm that adjusts the weights of a model in order to minimize its loss function. There are many different optimizers available in deep learning, each with its own strengths and weaknesses, and its choice depends on the specific problem and model being trained, and is often a matter of trial and error.   \n",
    "Different optimizers can affect the weights of a model in different ways because they use different algorithms to update the weights during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JL-tqSupov3N",
   "metadata": {
    "id": "JL-tqSupov3N"
   },
   "source": [
    "##### 5. Train the model with two different optimizers and see in a plot how the weights differ ([hint](https://pytorch.org/docs/stable/optim.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MypiUDohpQmV",
   "metadata": {
    "id": "MypiUDohpQmV"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k2T6q471WuLj",
   "metadata": {
    "id": "k2T6q471WuLj"
   },
   "source": [
    "#### Training with different initializations\n",
    "\n",
    "By default, linear layers are initialised with a uniform distribution bounded by $\\dfrac{1}{\\sqrt{in features}}$, while conv2d layers by a uniform distribution bouded by $\\dfrac{1}{\\sqrt{(in features) \\cdot k}}$ , where $k$ is the kernel size.\n",
    "\n",
    "By changing the default kernel initialization, the weights of the model vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ex4t8hHDZ9dR",
   "metadata": {
    "id": "Ex4t8hHDZ9dR"
   },
   "source": [
    "##### 6. Try to change the model weight initialization and see how they compare with the ones computed with the default one ([hint](https://pytorch.org/docs/stable/nn.init.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XTSi9Lpba6tu",
   "metadata": {
    "id": "XTSi9Lpba6tu"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
